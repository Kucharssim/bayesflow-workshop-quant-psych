---
title: "Deep Learning"
format:
    revealjs: 
        theme: default
        slide-number: true
        html-math-method: mathjax
        highlight-style: github
---

## What is Deep Learning



# Python frameworks

- [`PyTorch`](https://pytorch.org/)
- [`TensorFlow`](https://www.tensorflow.org/)
- [`JAX`](https://jax.readthedocs.io/en/latest/index.html)
- [`keras`](https://keras.io/)

## [`PyTorch`](https://pytorch.org/)

- created by Meta (formerly Facebook) 
- easy to learn
- focus on research prototypes
- models are not compiled


## [`TensorFlow`](https://www.tensorflow.org/)

- created by Google
- easy to learn
- focus on production
- models are compiled


## [`JAX`](https://jax.readthedocs.io/en/latest/index.html)

- created by Google
- pure, functional approach 
- JIT compiled
- fastest in runtime
- most difficult to learn

## [`keras`](https://keras.io/)

- created by Google
- API library
- Uses `PyTorch`, `TensorFlow`, or `jax` as a backend

```{.python filename="Python"}
import os
os.environ["KERAS_BACKEND"] = "jax" # torch, tensorflow
import keras
```

## [`keras`](https://keras.io/)

- created by Google
- API library
- Uses `PyTorch`, `TensorFlow`, or `jax` as a backend

```{.bash filename="Terminal"}
conda env config vars set KERAS_BACKEND=jax
```

<br>

```{.python filename="Python"}
import keras
```


## Tensors

- All of Deep Learning revolves around "Tensors"
- Similar to multidimensional arrays in `numpy`
- Additional features:
  - Stores values and gradients
  - Can be stored on GPUs (optional)

## Tensors

```{.python filename="Python"}
import os
os.environ["KERAS_BACKEND"] = "tensorflow"
import keras

x = keras.ops.zeros((16, 2))
x.shape # TensorShape([16, 2])
x.device # device:CPU:0
```

- First axis almost always `batch_size` (think "sample size")
- Other axes contextual (`timepoint`, `feature`, `variable`, `row`, `column`, etc)

# Neural networks

## The anatomy of neural networks

![](../figures/neural-network.svg){fig-align="center"}

## Neuron

Regression + non-linear activation 

![](../figures/neuron.svg){fig-align="center"}


## Perceptron

::::{.columns}

:::{.column width="70%"}
Multiple regressions + non-linear activations

$$
\begin{aligned}
z_k &= \sigma \Big(b_k + \sum_{j=1}^J W_{jk}x_j\Big)\\ 
z &= \sigma \Big(b + x W'\Big)
\end{aligned}
$$

:::

:::{.column width="30%"}
![](../figures/perceptron.svg){width=100%}
:::
::::

## Perceptron in code

```{.python filename="Python"}
import keras

network = keras.models.Sequential([
    keras.Input((3,)),
    keras.layers.Dense(5, activation="relu"),
])
network.summary()

x = keras.random.normal((100, 3))
x.shape # TensorShape([100, 3])

z = network(x)
z.shape # TensorShape([100, 5])
```

## Multi-Layer Perceptron

Multiple "layers" of perceptrons

![](../figures/neural-network.svg){fig-align="center"}



## Multi-Layer Perceptron

Function composition

$$
\begin{aligned}
z & = f(x) \text{ where } f = f_L \circ f_{L-1} \circ \dots \circ f_1 \\ 
z & = f_L(\dots(f_2(f_1(x)))) \\
\end{aligned}
$$

- $W_{jk}^l$: weight of the input $j$ to the output $k$ in the layer $l$
 
## Multi-Layer Perceptron in Code

```{.python filename="Python" code-line-numbers="3-9"}
import keras

network = keras.models.Sequential([
    keras.Input((3,)),
    keras.layers.Dense(4, activation="relu"),   # 3 inputs, 4 outputs
    keras.layers.Dense(4, activation="relu"),   # 4 inputs, 4 outputs
    keras.layers.Dense(2, activation="softmax") # 4 inputs, 2 outputs
])
network.summary()

x = keras.random.normal((100, 3))
x.shape # TensorShape([100, 3])

z = network(x)
z.shape # TensorShape([100, 2])
```

## Multi-Layer Perceptron in Code

```{.python filename="Python" code-line-numbers="11-16"}
import keras

network = keras.models.Sequential([
    keras.Input((3,)),
    keras.layers.Dense(4, activation="relu"),   # 3 inputs, 4 outputs
    keras.layers.Dense(4, activation="relu"),   # 4 inputs, 4 outputs
    keras.layers.Dense(2, activation="softmax") # 4 inputs, 2 outputs
])
network.summary()

x = keras.random.normal((100, 3))
x.shape # TensorShape([100, 3])

z = network(x) 
z.shape # TensorShape([100, 2])
```

# Activation functions

## Why activation functions?

- A composition of linear functions is itself a linear function
- Adding nonlinear activations introduces non-linearity
- $\rightarrow$ In theory, represent any non-linear function
- Often used for output range control


## What is an activation function?

:::{.incremental}
- Basic idea: Neuron "firing activity" based on its internal state
- Requirements:
  - Non-linearity (expressiveness)
  - Differentiability (training)
  - Efficiency (scalability)
:::

## Activation functions

- There are many
  - [https://keras.io/api/layers/activations/](https://keras.io/api/layers/activations/)
  - [https://arxiv.org/abs/2402.09092](https://arxiv.org/abs/2402.09092)

Here are some basic examples

## Activation functions

$$
\tanh{(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

## Activation functions

$$
\text{ReLU}(x) = \begin{cases} 0, x \leq 0 \\ x, x > 0\end{cases}
$$

## Activation functions

$$
\text{GELU}(x) = x \Phi(x)
$$

## Activation functions

$$
\text{softplus}(x) = \log(1 + e^x)
$$

## Activation functions

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

## Activation functions

$$
\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{J} e^{x_j}}
$$


# Training networks

## Loss function



## Optimization {.smaller}

:::{.incremental}
- Gradient based (e.g., Newton's method)
  - Second derivative of loss
  - Few steps to converge to an optimum
  - Each step is relatively slow
  - Best for small data
- Stochastic gradient descent (SGD)
  - First derivative of loss
  - Many steps to converge to an optimum
  - Each step is relatively cheap
  - Best for big data
- Other methods
  - BFGS, Gauss-Newton, Adagrad, Particle swarm, differential evolution,... 
:::

## Gradient descent

$$
\theta_{n+1} = \theta_n - \gamma \Delta_\theta \mathcal{L}(x; \theta_n) 
$$

- $\theta_{n+1}$: New network weights
- $\theta_n$: Current network weights
- $\gamma$: Learning rate
- $\Delta_\theta$: Gradient (matrix of partial derivatives w.r.t network weights)
- $\mathcal{L}$: Loss function
- $x$: Data

## Backpropagation

## Adam

## Training regimes

1. Offline
    - "In memory" - load at once (small to moderate data)
    - "On disk" - load from disk on demand (large data)
2. Online
    - Generate data on the fly
3. Distributed training
    - Big data

# Training tricks

## Learning rate scheduling

## Regularization

## Early stopping

## Dropout

## Batch normalization

## Gradient clipping


