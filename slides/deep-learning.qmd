---
title: "Deep Learning"
format:
    revealjs: 
        theme: default
        slide-number: true
        html-math-method: mathjax
        highlight-style: github
---

## What is "Deep Learning"?

# Python frameworks

- [`PyTorch`](https://pytorch.org/)
- [`TensorFlow`](https://www.tensorflow.org/)
- [`JAX`](https://jax.readthedocs.io/en/latest/index.html)
- [`keras`](https://keras.io/)

## [`PyTorch`](https://pytorch.org/)

- created by Meta (formerly Facebook) 
- easy to learn
- focus on research prototypes
- models are not compiled


## [`TensorFlow`](https://www.tensorflow.org/)

- created by Google
- easy to learn
- focus on production
- models are compiled


## [`JAX`](https://jax.readthedocs.io/en/latest/index.html)

- created by Google
- pure, functional approach 
- JIT compiled
- fastest in runtime
- most difficult to learn

## [`keras`](https://keras.io/)

- created by Google
- API library
- Uses `PyTorch`, `TensorFlow`, or `jax` as a backend

```{.python filename="Python"}
import os
os.environ["KERAS_BACKEND"] = "jax" # torch, tensorflow
import keras
```

## [`keras`](https://keras.io/)

- created by Google
- API library
- Uses `PyTorch`, `TensorFlow`, or `jax` as a backend

```{.bash filename="Terminal"}
conda env config vars set KERAS_BACKEND=jax
```

<br>

```{.python filename="Python"}
import keras
```


## Tensors

- All of Deep Learning revolves around "Tensors"
- Similar to multidimensional arrays in `numpy`
- Additional features:
  - Stores values and gradients
  - Can be stored on GPUs (optional)

## Tensors

```{.python filename="Python"}
import torch

x = torch.zeros((16, 2))

x.shape        # torch.Size([16, 2])
str(x.device)  # 'cpu'
x.grad is None # True
```

- First axis almost always `batch_size` (think "sample size")
- Other axes contextual (`timepoint`, `feature`, `variable`, `row`, `column`, etc)

# Neural networks

## A network

```{dot}
digraph {
  rotate=90;
  x1 [label="" shape="circle" style="filled" fillcolor="coral1"]
  x2 [label="" shape="circle" style="filled" fillcolor="coral1"]
  
  z11 [label="" shape="circle" style="filled" fillcolor="lightgray"]
  z12 [label="" shape="circle" style="filled" fillcolor="lightgray"]
  z13 [label="" shape="circle" style="filled" fillcolor="lightgray"]
  
  z21 [label="" shape="circle" style="filled" fillcolor="lightgray"]
  z22 [label="" shape="circle" style="filled" fillcolor="lightgray"]
  z23 [label="" shape="circle" style="filled" fillcolor="lightgray"]
  
  y1 [label="" shape="circle" style="filled" fillcolor="lightblue"]
  y2 [label="" shape="circle" style="filled" fillcolor="lightblue"]
  
  x1, x2 -> z11, z12, z13
  z11, z12, z13 -> z21, z22, z23
  z21, z22, z23 -> y1, y2
}
``` 



## The anatomy of neural networks

```{ojs}
d3 = require("d3@7")
```

```{ojs}
mlp = {
    // Declare the chart dimensions and margins.
    const width = 1000;
    const height = 500;

    const layers = [3, 4, 4, 2]; // 2 input nodes, 2 hidden layers with 4 nodes each, 1 output node
    const layerSpacing = width / (layers.length + 1);
    const nodeRadius = 30;

    // Create the SVG container.
    const svg = d3.create("svg")
        .attr("width", width)
        .attr("height", height);

    // Generate nodes
    const nodes = [];
    layers.forEach((count, i) => {
    for (let j = 0; j < count; j++) {
        nodes.push({
        layer: i,
        x: (i + 1) * layerSpacing,
        y: height / (count + 1) * (j + 1),
        type: i === 0 ? "input" : (i === layers.length -1 ? "output" : "hidden") // node type
        });
    }
    });

    // Generate links between nodes
    const links = [];
    for (let i = 0; i < layers.length - 1; i++) {
    const currentLayerNodes = nodes.filter(d => d.layer === i);
    const nextLayerNodes = nodes.filter(d => d.layer === i + 1);

    currentLayerNodes.forEach(source => {
        nextLayerNodes.forEach(target => {
        links.push({ source, target });
        });
    });
    }


    // Draw links
    svg.selectAll("line")
    .data(links)
    .join("line")
    .attr("x1", d => d.source.x)
    .attr("y1", d => d.source.y)
    .attr("x2", d => d.target.x)
    .attr("y2", d => d.target.y)
    .attr("stroke", "#000000")
    .attr("stroke-width", 2);


    // Color mapping for node types
    const colorMap = {
        input: "#FF7F50",   // Green for input layer
        hidden: "#D3D3D3",  // Blue for hidden layers
        output: "#ADD8E6"    // Orange for output layer
    };

    // Draw nodes
    svg.selectAll("circle")
        .data(nodes)
        .join("circle")
        .attr("cx", d => d.x)
        .attr("cy", d => d.y)
        .attr("r", nodeRadius)
        .attr("fill", d => colorMap[d.type])
        .attr("stroke", "#000000")
        .attr("stroke-width", 2);

  // Return the SVG element.
  return svg.node();
}
```

## Neuron

- A regression with a non-linear activation
- $J$-dimensional input, $x$
- 1-dimensional output, $z$

$$
z = \sigma\Big(b + \sum_{j=1}^J x_j W_j \Big)
$$

## Perceptron

- Multiple regressions made by a collection of neurons
- $J$-dimensional input, $x$
- $K$-dimensional output, $z$

$$
\begin{aligned}
z_k &= \sigma \Big(b_k + \sum_{j=1}^J W_{jk}x_j\Big) \text{ for } k \in \{1, \dots, K\}\\ 
z &= \sigma \Big(b + x W'\Big)
\end{aligned}
$$


## Multi-Layer Perceptron

- $L$ layers of perceptrons
- A composition of functions

$$
\begin{aligned}
z & = f(x) \text{ where } f = f_L \circ f_{L-1} \circ \dots \circ f_1 \\ 
z & = f_L(\dots(f_2(f_1(x)))) \\
\end{aligned}
$$

- $W_{jk}^l$: weight of the input $j$ to the output $k$ in the layer $l$
 
## Multi-Layer Perceptron in Code

```{.python filename="Python" code-line-numbers="4-11"}
import keras
import numpy as np

network = keras.models.Sequential([
    keras.Input((3,)),
    keras.layers.Dense(4, activation="relu"),   # 3 inputs, 4 outputs
    keras.layers.Dense(4, activation="relu"),   # 4 inputs, 4 outputs
    keras.layers.Dense(2, activation="softmax") # 4 inputs, 2 outputs
])

network.summary()

x = np.random.normal(size=(100, 3))
x.shape # (100, 3)

z = network(x)
z.shape # (100, 2)
```

## Multi-Layer Perceptron in Code

```{.python filename="Python" code-line-numbers="13-17"}
import keras
import numpy as np

network = keras.models.Sequential([
    keras.Input((3,)),
    keras.layers.Dense(4, activation="relu"),   # 3 inputs, 4 outputs
    keras.layers.Dense(4, activation="relu"),   # 4 inputs, 4 outputs
    keras.layers.Dense(2, activation="softmax") # 4 inputs, 2 outputs
])

network.summary()

x = np.random.normal(size=(100, 3))
x.shape # (100, 3)

z = network(x) 
z.shape # (100, 2)
```

# Activation functions

## Why activation functions?

- A composition of linear functions is itself a linear function
- Adding nonlinear activations introduces non-linearity
- $\rightarrow$ In theory, represent any non-linear function
- Often used for output range control


## What is an activation function?

:::{.incremental}
- Basic idea: Neuron "firing activity" based on its internal state
- Requirements:
  - Non-linearity (expressiveness)
  - Differentiability (training)
  - Efficiency (scalability)
:::

## Activation functions

- There are many
  - [https://keras.io/api/layers/activations/](https://keras.io/api/layers/activations/)
  - [https://arxiv.org/abs/2402.09092](https://arxiv.org/abs/2402.09092)

Here are some basic examples

## Activation functions

$$
\tanh{(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

## Activation functions

$$
\text{ReLU}(x) = \begin{cases} 0, x \leq 0 \\ x, x > 0\end{cases}
$$

## Activation functions

$$
\text{GELU}(x) = x \Phi(x)
$$

## Activation functions

$$
\text{softplus}(x) = \log(1 + e^x)
$$

## Activation functions

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

## Activation functions

$$
\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{J} e^{x_j}}
$$


# Training networks

## Loss function



## Optimization {.smaller}

:::{.incremental}
- Gradient based (e.g., Newton's method)
  - Second derivative of loss
  - Few steps to converge to an optimum
  - Each step is relatively slow
  - Best for small data
- Stochastic gradient descent (SGD)
  - First derivative of loss
  - Many steps to converge to an optimum
  - Each step is relatively cheap
  - Best for big data
- Other methods
  - BFGS, Gauss-Newton, Adagrad, Particle swarm, differential evolution,... 
:::

## Gradient descent

$$
\theta_{n+1} = \theta_n - \gamma \Delta_\theta \mathcal{L}(x; \theta_n) 
$$

- $\theta_{n+1}$: New network weights
- $\theta_n$: Current network weights
- $\gamma$: Learning rate
- $\Delta_\theta$: Gradient (matrix of partial derivatives w.r.t network weights)
- $\mathcal{L}$: Loss function
- $x$: Data

## Backpropagation

## Adam

## Training regimes

1. Offline
    - "In memory" - load at once (small to moderate data)
    - "On disk" - load from disk on demand (large data)
2. Online
    - Generate data on the fly
3. Distributed training
    - Big data

# Training tricks

## Learning rate scheduling

## Regularization

## Early stopping

## Dropout

## Batch normalization

## Gradient clipping


