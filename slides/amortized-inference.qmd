---
title: "Amortized inference"
format:
    revealjs: 
        theme: default
        slide-number: true
        html-math-method: mathjax
        highlight-style: github
---

## Recap

### Problem

We want to approximate $p(\theta \mid x)$ or $p(x \mid \theta)$

$$
p(\theta \mid x) = \frac{p(\theta) \times p(x \mid \theta)}{p(x)}
$$

### Solution

Approximate using generative neural networks

## General solution

1. Define a statistical model $p(\theta, x)$
2. Generative neural network to $q_\phi(\theta \mid x)$
3. Train the network
   1. Sample from $(\theta^{(s)}, x^{(s)}) \sim p(\theta, x)$
   2. Optimize weights $\phi$ so that $q_\phi(\theta \mid x) \approx p(\theta \mid x)$


## Normalizing Flows

$$
\begin{aligned}
\hat{\phi} = \operatorname*{argmin}_{\phi} & \mathbb{E}_{x \sim p(x)} \mathbb{KL}(p(\theta \mid x) || q_\phi(\theta \mid x)) = \\
= \operatorname*{argmin}_{\phi} & \mathbb{E}_{x \sim p(x)} \mathbb{E}_{\theta \sim p(\theta \mid x)} \log \frac{p(\theta \mid x)}{q_\phi(\theta \mid x)} \approx \\
\approx \operatorname*{argmin}_{\phi} & \sum_{s=1}^S \log p(\theta^{(s)} \mid x^{(s)}) - \log q_\phi(\theta^{(s)} \mid x^{(s)}) \propto \\
\propto \operatorname*{argmin}_{\phi} & - \sum_{s=1}^S \log q_\phi(\theta^{(s)} \mid x^{(s)})
\end{aligned}
$$

## Normalizing Flows

$$
\hat{\phi} = \operatorname*{argmin}_{\phi} - \sum_{s=1}^S \log q_\phi(\theta^{(s)} \mid x^{(s)})
$$

$$
q_\phi(\theta^{(s)} \mid x^{(s)}) = p_z(f_\phi(\theta^{(s)} \mid x)) \det{J}_\phi(\theta^{(s)})
$$

## Flow matching - intuition

- Sample from $(\theta, x) \sim p(\theta, x)$
- Train a flow matching network to transport $p_0(\theta) \rightarrow p_1(\theta \mid x)$
- Network learns $p_1(\theta \mid x) \approx p(\theta \mid x)$

## Data of different dimensions



# Working with `bayesflow`

## Bayesflow

- Bayesflow is a Python library
- Built on `keras`
  - Can run on `TensorFlow`, `JAX`, or `PyTorch` as a backend
- Implementation of common neural architectures to make ABI easier
- Helper functions for simulation, configuration, training, validation, diagnostics,...

## Ingredients {.smaller}

- Simulator
  - Simulate data from the statistical model 
- Adapter
  - Configure the data into a format suitable for neural networks
  - Reshape, standardize
- Summary network (optional)
  - Get data embedding
- Inference network
  - Approximate the posterior
- Approximator
  - Combine adapter, summary net, inference net 

## Workflow

1. Define a simulator (statistical generative model)
2. Define approximator (using neural networks)
3. Train the networks using simulated data
4. Network diagnostics
5. Apply to real data
6. Inference validation

# Define the statistical model

## Define a simulator

- Define a "statistical" model in terms of a simulator
- `bayesflow` expects "batched" simulations

```{.python filename="Python"}
class Simulator(bf.simulators.Simulator):
    def sample(self, batch_size):
        theta = np.random.beta(1, 1, size=batch_size)
        x = np.random.binomial(n=10, p=theta)
        return dict(theta=theta, x=x)

simulator = Simulator()
dataset = simulator.sample(100)
```


## Define a simulator

- Define a "statistical" model in terms of a simulator
- `bayesflow` expects "batched" simulations
- Convenient interface for "auto batching"

```{.python filename="Python"}
def prior():
    return dict(theta = np.random.beta(1, 1))

def likelihood(theta):
    return dict(x = np.random.binomial(n=10, p=theta))

simulator = bf.make_simulator([prior, likelihood])

simulator.sample(10)
```

# Define neural approximator

## Define approximator

```{.python filename="Python"}
approximator = bf.app
```

# Train neural networks

## Network training

# Validate the model

## Goals

1. Is the neural approximator doing a good job at approximating the true posterior?
    - Computational faithfullness
2. How much can we expect to learn given data?
    - Parameter recovery
    - Posterior contraction
    - Posterior z-score


## Procedure

1. Draw fresh validation data from the simulator
2. Extract the parameter samples from the prior
3. Sample from the posterior

```{.python filename="Python"}
dataset = simulator.sample(1_000)
prior = {k: v if k in param_keys for k, v in dataset.items()}
posterior = approximator.sample(500, conditions = dataset)
```

## Simulation-based calibration (SBC)

- Testing computational faithfullness of a posterior approximator
- Posterior distribution averaged over prior predictive distribution is the same as the prior distribution

$$
p(\theta) = \int \int p(\theta \mid \tilde{y}) \underbrace{p(\tilde{y} \mid \tilde{\theta}) p(\tilde{\theta})}_{\text{Prior predictives}} d\tilde{\theta} d \tilde{y}
$$

## Simulation-based calibration 

::::{.columns}
:::{.column width=50%}
### Simulate
$$
\begin{aligned}
\theta^{\text{sim}} & \sim p(\theta) \\
y^{\text{sim}} &\sim p(y \mid \theta^{\text{sim}})
\end{aligned}
$$
:::

:::{.column width=50%}
### By symmetry
$$
\begin{aligned}
(\theta^{\text{sim}}, y^{\text{sim}}) & \sim p(\theta, y) \\
\theta^{\text{sim}} &\sim p(\theta \mid y^{\text{sim}})
\end{aligned}
$$
:::
::::

### Compute
$$
\begin{aligned}
\theta_1, \dots, \theta_M & \sim q(\theta \mid y^{\text{sim}}) \\
\end{aligned}
$$

If $q(\theta \mid y^{\text{sim}}) = p(\theta \mid y^{\text{sim}})$, then the rank statistic of $\theta^{\text{sim}}$ is uniform.

## Simulation-based calibration

1. Draw $N$ prior predictive datasets $(\theta_i^{\text{sim}}, y_i^{\text{sim}}) \sim p(\theta, y)$
2. For each data set $y_i$, draw $M$ samples from the approximate posterior: $\theta_{ij} \sim q(\theta \mid y_i^{\text{sim}})$
3. Calculate rank statistic $r_i = \sum_{j=1}^M \text{I}\big(\theta_{ij} < \theta_i^{\text{sim}}\big)$ 

## Visualizing SBC

1. Histograms
2. ECDF 
3. ECDF difference

## SBC Histograms

```{.python filename="Python"}
fig=bf.diagnostics.calibration_histogram(
    estimates=posterior, 
    targets=prior)
```

</br>

![](../figures/sbc/histogram.png){fig-align="center"}

## SBC ECDF

```{.python filename="Python"}
fig=bf.diagnostics.calibration_ecdf(
    estimates=posterior, 
    targets=prior)
```

</br>

![](../figures/sbc/ecdf.png){fig-align="center"}

## SBC ECDF Difference

```{.python filename="Python"}
fig=bf.diagnostics.calibration_ecdf(
    estimates=posterior, 
    targets=prior,
    difference=True)
```

</br>

![](../figures/sbc/ecdf-diff.png){fig-align="center"}

## Posterior z-score

- How well does the estimated posterior mean match the true parameter used for simulating the data?

$$
z = \frac{\text{mean}(\theta_i) - \theta^{\text{sim}}}{\text{sd}(\theta_i)}
$$

## Posterior contraction

- How much uncertainty is removed after updating the prior to posterior?

$$
\text{contraction} = 1 - \frac{\text{sd}(\theta_i)}{\text{sd}(\theta^{\text{sim}})}
$$


## Z-score vs. contraction plot

```{.python filename="Python"}
fig=bf.diagnostics.plots.z_score_contraction(
    estimates=posterior, 
    targets=prior)
```

</br>

![](../figures/z-score_contraction.png){fig-align="center"}

## Parameter recovery

- Plot the true parameter values $\theta^{\text{sim}}$ against a posterior point estimate (mean, median)

```{.python filename="Python"}
fig=bf.diagnostics.plots.recovery(estimates=posterior, targets=prior)
```

![](../figures/recovery.png){fig-align="center"}

# Inference

## Obtain posterior samples

```{.python filename="Python"}
data = dict(y = ...)
posterior = approximator.sample(1000, conditions=data)

fig=bf.diagnostics.plots.pairs_posterior(estimates=posterior)
```

## Simulation gap / distribution shift

- Model might be misspecified
- The data is very different

## Posterior predictive checks

