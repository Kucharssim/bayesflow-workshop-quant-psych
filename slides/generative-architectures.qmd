---
title: "Generative Neural Networks"
format:
    revealjs: 
        theme: default
        slide-number: true
        html-math-method: mathjax
        highlight-style: github
        include-in-header:
            text: |
                <script src="https://cdn.jsdelivr.net/npm/mermaid@11.2.1/dist/mermaid.min.js"></script>
---


## Probabilistic generative models

We have samples of a random variable X, and we are interested in learning (approximating) it's probability distribution $p_X(X)$

Uses:

- **Evaluate** the density $p_X(X)$
- **Sample** new data from the distribution, $X \sim p_X(X)$

## Mixture model

- The target distribution is modeled as a weighted sum of multiple simpler (e.g., Gaussian) distributions
  - $p_X(X) = \sum_k^K w_k p_k(X)$
- Sampling and evaluating is straightforward
- Theoretically can represent any distribution
- Practically, does not scale well

## Many architectures

- Markov random fields [@li2009markov]
- Generative adversarial networks [GAN, @goodfellow2014generative]
- Variational autoencoders [VAE, @kingma2013auto]
- Diffusion models [@song2020score]
- Consistency models [@song2023consistency]
- **Normalizing flows** [@kobyzev2020normalizing;@papamakarios2021normalizing]
- **Flow matching** [@lipman2022flow]

# Normalizing flows

## Normalizing flows

Generative models build on invertible transformations of random variables

General characteristics:

- Efficient to evaluate $p_X(x)$
- Efficient fo sample from $p_X$
- Expressive (flexible)
- Can represent high-dimensional probability distributions
- Easy to train (standard ML methods)

## Change of variables

Transformations of random variables are described by "change of variables"

- $Z \sim p_Z(z)$
- $Z = f(X)$ (or $X = f^{-1}(Z)$)
- $p_X(x)?$


Express the density of $X$ using the density of $Z$ and the transform $f$

## Change of variables - intuition

:::: {.columns}

::: {.column width="50%"}
$$Z \sim \text{Uniform}(0, 1)$$
:::

::::

## Change of variables - intuition

:::: {.columns}

::: {.column width="50%"}
$$Z \sim \text{Uniform}(0, 1)$$

![](../figures/uniform.svg)
:::

::::


## Change of variables - intuition

:::: {.columns}

::: {.column width="50%"}
$$Z \sim \text{Uniform}(0, 1)$$

![](../figures/uniform.svg)
:::

::: {.column width="50%"}
$$X = 2Z - 1$$
:::
::::

## Change of variables - intuition

:::: {.columns}

::: {.column width="50%"}
$$Z \sim \text{Uniform}(0, 1)$$

![](../figures/uniform.svg)
:::

::: {.column width="50%"}
$$X = 2Z - 1$$

![](../figures/uniform-rescaled.svg)
:::
::::

## Change of variables - affine transform

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times b$

## Change of variables - affine transform

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times a$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$

## Change of variables - affine transform

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times a$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = p_Z(f(x)) \times a$

## Change of variables - affine transform

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times a$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} f(x)^2 \right) \times a$

## Change of variables - affine transform

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times a$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2 \right) \times a$

## Change of variables - affine transform

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times a$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2 \right) \times \frac{1}{\sigma}$

## Change of variables - affine transform

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times a$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2 \right)$


## Change of variables - more formally

$$
p_X(x) = p_Z(f(x))  \left| \frac{d}{dx} f(x) \right|
$$


## Change of variables - more formally

$$
p_X(x) = p_Z(f(x))  \left| \frac{d}{dx} f(x) \right|
$$

- $f: Z = \log(X)$
- $\frac{d}{dx} f(x) = \frac{d}{dx} \log(x) = \frac{1}{x}$

## Change of variables - more formally

$$
p_X(x) = p_Z(f(x))  \left| \frac{d}{dx} f(x) \right|
$$

- $f: Z = \log(X)$
- $\frac{d}{dx} f(x) = \frac{d}{dx} \log(x) = \frac{1}{x}$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $p_X(x) = \frac{1}{x\sqrt{2\pi}} \exp\left(-\frac{1}{2} \log(x)^2\right)$

## Change of variables - multivariate

$$
p_X(x) = p_Z(f(x)) \left| \det{J}_f(x) \right|
$$

$$
J_f(x) = \begin{bmatrix}
\frac{\partial z_1}{\partial x_1} & \dots & \frac{\partial z_1}{\partial x_K} \\
\vdots & \ddots & \vdots \\
\frac{\partial z_K}{\partial x_1} & \dots & \frac{\partial z_K}{\partial x_K}
\end{bmatrix}
$$


## Change of variables - multivariate

$$f\left(\begin{bmatrix}x_1 \\ x_2\end{bmatrix}\right) = \begin{bmatrix} x_1^2 x_2 \\ 3x_1 + \sin x_2 \end{bmatrix} = \begin{bmatrix}z_1 \\ z_2\end{bmatrix}$$

$$J_f(x) = \begin{bmatrix}
\frac{\partial z_1}{\partial x_1} & \frac{\partial z_1}{\partial x_2} \\
\frac{\partial z_2}{\partial x_1} & \frac{\partial z_2}{\partial x_2}
\end{bmatrix} = \begin{bmatrix} 2x_1x_2 & x_1^2 \\ 3 & \cos x_2 \end{bmatrix}
$$


## Normalizing flows

- Given samples $X$
- Find $f$ such that $f(X) = Z \sim \text{Normal}(0, \text{I})$
- "$f$ *normalizes* $X$"


## Normalizing flows

- Given samples $X$
- Find $f$ such that $f(X) = Z \sim \text{Normal}(0, \text{I})$
- "$f$ *normalizes* $X$"

:::: {.columns}

:::{.column width="60%"}
#### Density evaluation 

$$
p_X(x) = p_Z(f(x)) \left| \det{J}_f(x) \right|
$$
:::

:::{.column width=40%}

:::
::::

## Normalizing flows

- Given samples $X$
- Find $f$ such that $f(X) = Z \sim \text{Normal}(0, \text{I})$
- "$f$ *normalizes* $X$"

:::: {.columns}

:::{.column width="60%"}
#### Density evaluation

$$
p_X(x) = p_Z(f(x)) \left| \det{J}_f(x) \right|
$$
:::

:::{.column width="40%"}
#### Sampling

$$
\begin{aligned}
Z &\sim \text{Normal}(0, \text{I}) \\ 
X & = f^{-1}(Z)
\end{aligned}
$$
:::
::::

## Training

Via maximum likelihood (or rather: negative log likelihood)

$$
\arg \min_\phi - \sum_{i=1}^n \log p_Z(f(x_i \mid \phi)) + \log \left| \det{J}_f(x_i \mid \phi) \right|,
$$
where $\phi$ are trainable weights of a neural network that implements $f$.

## Challenge {.incremental}

Flow $f$ is a parametric function that:

  - is invertible ($f^{-1}$ exists)
  - is differentiable
  - has a Jacobian determinant $\left|\det{J}_f(x_i \mid \phi) \right|$ that is computationally efficient
  - expressive enough to represent non-trivial distributions

## Flow composition

Invertible and differentiable functions are "closed" under composition

$$
f = f_L \circ f_{L-1} \circ \dots \circ f_1 \\ 
$$

::: {#fig-flow-composition layout="[[-10, 1, -10, 1, -10, 1, -10], [10, 1, 10, 1, 10, 1, 10]]"}

$f_1$ 

$f_2$ 

$f_3$ 


![](../figures/normalizing-flow/x_1.svg)

$\rightarrow$ 

![](../figures/normalizing-flow/x_2.svg)

$\rightarrow$

![](../figures/normalizing-flow/x_3.svg)

$\rightarrow$

![](../figures/normalizing-flow/x_4.svg)

Flow composition in forward direction
:::

## Flow composition - inverse

To invert a flow composition, we invert individual flows and run them in the opposite order

$$
f^{-1} = f_1^{-1} \circ f_2 ^{-1} \circ \dots \circ f_L^{-1} \\ 
$$

::: {#fig-flow-composition layout="[[-10, 1, -10, 1, -10, 1, -10], [10, 1, 10, 1, 10, 1, 10]]"}

$f_1^{-1}$ 

$f_2^{-1}$ 

$f_3^{-1}$ 


![](../figures/normalizing-flow/x_1.svg)

$\leftarrow$ 

![](../figures/normalizing-flow/x_2.svg)

$\leftarrow$

![](../figures/normalizing-flow/x_3.svg)

$\leftarrow$

![](../figures/normalizing-flow/x_4.svg)

Flow composition in backward (inverse) direction
:::

## Flow composition - Jacobian

- Chain rule
$$
\left| \det{J}_f(x) \right| = \left| \det \prod_{l=1}^L J_{f_l}(x)\right| = \prod_{l=1}^L \left| \det{J}_{f_l}(x)\right|
$$

- if we have a Jacobian for each individual transformation, then we have a Jacobian for their composition
$$
\arg \min_\phi \sum_{i=1}^n \log p_Z(f(x_i \mid \phi)) + \sum_{l=1}^L \log \left| \det{J}_{f_l}(x_i \mid \phi) \right|
$$

## Linear flow

:::{.incremental}
$$
f(x) = Ax + b
$$

- inverse: $f^{-1}(z) = A^{-1}(x - b)$
- Jacobian: $\left| \det{J}_f(x) \right| = \left| \det{A} \right|$

- Limitations:
  1. Not expressive (composition of linear functions is a linear function)
  2. Jacobian/inverse may be in $\mathcal{O}(p^3)$
:::

## Coupling flows

- *Increasing* expresiveness while potentially *decreasing* computational costs
- A *coupling flow* is a way to construct non-linear flows

1. Split the data in two disjoint subsets: $x = (x_A, x_B)$
2. Compute parameters conditionally on one subset: $\theta(x_A)$
3. Apply transformation to the other subset: $z_B = f(x_B \mid \theta(x_A))$
4. Concatenate $z = (x_A, z_B)$

## Coupling flow: forward

![](../figures/coupling-layer-forward.svg){fig-align="center"}

## Coupling flow: backward

![](../figures/coupling-layer-backward.svg){fig-align="center"}

## Coupling flow trick

- Jacobian

$$ J_f = 
\begin{bmatrix}
\text{I} & 0 \\
\frac{\partial}{\partial x_A}f(x_B \mid \theta(x_A)) & J_f(x_B \mid \theta(x_A))
\end{bmatrix}
$$

- Determinant

$$
\det{J}_f = \det(\text{I}) \times \det{J}_f(x_B \mid \theta(x_A)) = \det{J}_f(x_B \mid \theta(x_A))
$$


## Coupling flow trick

- $f(x_B\mid\theta(x_A))$ needs to be differentiable and invertible
  - easy to calculate determinant Jacobian... 
- $\theta(x_A)$ can be arbitrarily complex
  - non-linear,...
  - $\rightarrow$ neural network

- Stack multiple coupling blocks and permute $x_{A}$ and $x_B$


## Affine coupling

:::{.incremental}
- $\theta(x_A)$: Trainable coupling networks
  - Output: Shift $\mu$ and scale $\sigma$ 
- Linear (affine) transform function $f(x_B\mid\theta(x_A)) = \frac{x_B - \mu(x_A)}{\sigma(x_A)}$

- log det Jacobian: $-\log{\sigma(x_A)}$
:::

## Spline coupling [@muller2019neural]


::::{.columns}

:::{.column width=50%}
- Transformation: Splines
  - "Piecewise polynomials"
- More expressive
- Easier to overfit
- Slower at training and inference
:::

:::{.column width=50%}

![Figure from @durkan2019neural](../figures/spline-transform.png)

:::


::::


## Exercise - Moons

Build your own affine coupling normalizing flow!

:::: {.columns}

::: {.column width="50%"}
### Forward

{{< video ../exercises/normalizing-flow-forward.mp4 width="500" height="350">}}
:::

::: {.column width="50%"}
### Backward

{{< video ../exercises/normalizing-flow-backward.mp4 width="500" height="350">}}
:::

::::

## Idea

- Normalizing flows transform X into Z in a set of discrete steps
- But why not take one smooth/continuous transformation?

# Flow matching

## Flow matching

- Defines a flow that transforms a distribution over time
  - $p_{t=0} = p_z$ - Base distribution 
  - $p_{t=1} = p_x$ - Data distribution

![](../figures/flow-matching/two-dists.svg){fig-align="center"}

## Flow matching

- Flow defines position $X_t = \phi_t(X_0)$
- Time dependent vector field: $\frac{d}{dt} \phi_t(x) = u_t(\phi_t(x))$
- $X_0 \sim p_0(X_0)$
- $X_t = \phi_t(X_0) \sim p_t(X_t)$

![](../figures/flow-matching/two-dists-path.svg){fig-align="center"}

## Flow matching

$$
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t, X_t}|| u_\theta(X_t \mid t) - u(X_t \mid t) ||^2, t\sim\text{Uniform}(0,1), X_t \sim p_t(X_t)
$$

![](../figures/flow-matching/two-dists-path.svg){fig-align="center"}

## Flow matching

$X_t = (1-t) X_0 + t X_1$ 

## Flow matching

- Define a "probability path" $p_t(x) = \int p(x \mid z) p(z) dz$

## Flow matching

{{< video ../exercises/flow-matching-datasaurus.mp4 width="800">}}


