---
title: "Generative architectures"
format:
    revealjs: 
        theme: default
        slide-number: true
        html-math-method: mathjax
        include-in-header:
            text: |
                <script src="https://cdn.jsdelivr.net/npm/mermaid@11.2.1/dist/mermaid.min.js"></script>
---

## Probabilistic generative models

We have samples of a random variable X, and we are interested in learning (approximating) it's probability distribution $p_X(X)$

Uses:

- **Evaluate** the density $p_X(X)$
- **Sample** new data from the distribution, $X \sim p_X(X)$

## Example: Mixture model

- The target distribution is modeled as a weighted sum of multiple simpler (e.g., Gaussian) distributions
  - $p_X(X) = \sum_k^K w_k p_k(X)$
- Sampling and evaluating is straightforward
- Theoretically can represent any distribution
- Practically, does not scale well

## Other architectures

- Markov random fields
- Generative adversarial networks (GAN)
- Variational autoencoders (VAE)
- Diffusion models
- **Normalizing flows**
- **Flow matching**

# Normalizing flows

## Normalizing flows

Generative models build on invertible transformations of random variables

General characteristics:

- Efficient to evaluate $p_X(x)$
- Efficient fo sample from $p_X$
- Expressive (flexible)
- Can represent high-dimensional probability distributions
- Easy to train (standard ML methods)
<!-- - Transform a (complex) data distribution $p_X(X)$ into a simple (analytic) distribution -->

## Change of variables

Transformations of random variables are described by "change of variables"

- $Z \sim p_Z(z)$
- $Z = f(X)$ (or $X = f^{-1}(Z)$)
- $p_X(x)?$


Express the density of $X$ using the density of $Z$ and the transform $f$

## Change of variables - intuition {auto-animate=true}

:::: {.columns}

::: {.column width="50%"}
$$Z \sim \text{Uniform}(0, 1)$$
:::

::::

## Change of variables - intuition {auto-animate=true}

:::: {.columns}

::: {.column width="50%"}
$$Z \sim \text{Uniform}(0, 1)$$

![](../figures/uniform.svg)
:::

::::


## Change of variables - intuition {auto-animate=true}

:::: {.columns}

::: {.column width="50%"}
$$Z \sim \text{Uniform}(0, 1)$$

![](../figures/uniform.svg)
:::

::: {.column width="50%"}
$$X = 2Z - 1$$
:::
::::

## Change of variables - intuition {auto-animate=true}

:::: {.columns}

::: {.column width="50%"}
$$Z \sim \text{Uniform}(0, 1)$$

![](../figures/uniform.svg)
:::

::: {.column width="50%"}
$$X = 2Z - 1$$

![](../figures/uniform-rescaled.svg)
:::
::::

## Change of variables - affine transform {auto-animate=true}

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times b$

## Change of variables - affine transform {auto-animate=true}

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times b$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$

## Change of variables - affine transform {auto-animate=true}

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times a$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = p_Z(f(x)) \times a$

## Change of variables - affine transform {auto-animate=true}

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times b$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} f(x)^2 \right) \times a$

## Change of variables - affine transform {auto-animate=true}

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times b$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2 \right) \times a$

## Change of variables - affine transform {auto-animate=true}

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times b$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2 \right) \times \frac{1}{\sigma}$

## Change of variables - affine transform {auto-animate=true}

- $f: Z = a X + b$
  - shift by $b$: no effect
  - scale by a constant $a$: multiply by $a$
  - $p_X(x) = p_Z(f(x)) \times b$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $f: Z = \frac{(X - \mu)}{\sigma}$
- $p_X(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2 \right)$


## Change of variables - more formally {auto-animate=true}

$$
p_X(x) = p_Z(f(x))  \left| \frac{d}{dx} f(x) \right|
$$


## Change of variables - more formally {auto-animate=true}

$$
p_X(x) = p_Z(f(x))  \left| \frac{d}{dx} f(x) \right|
$$

- $f: Z = \log(X)$
- $\frac{d}{dx} f(x) = \frac{d}{dx} \log(x) = \frac{1}{x}$

## Change of variables - more formally {auto-animate=true}

$$
p_X(x) = p_Z(f(x))  \left| \frac{d}{dx} f(x) \right|
$$

- $f: Z = \log(X)$
- $\frac{d}{dx} f(x) = \frac{d}{dx} \log(x) = \frac{1}{x}$

- $p_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 \right)$
- $p_X(x) = \frac{1}{x\sqrt{2\pi}} \exp\left(-\frac{1}{2} \log(x)^2\right)$

## Change of variables - multivariate

$$
p_X(x) = p_Z(f(x)) \left| \det{J}_f(x) \right|
$$

$$
J_f(x) = \begin{bmatrix}
\frac{\partial z_1}{\partial x_1} & \dots & \frac{\partial z_1}{\partial x_K} \\
\vdots & \ddots & \vdots \\
\frac{\partial z_K}{\partial x_1} & \dots & \frac{\partial z_K}{\partial x_K}
\end{bmatrix}
$$


## Change of variables - multivariate

$$f\left(\begin{bmatrix}x_1 \\ x_2\end{bmatrix}\right) = \begin{bmatrix} x_1^2 x_2 \\ 3x_1 + \sin x_2 \end{bmatrix} = \begin{bmatrix}z_1 \\ z_2\end{bmatrix}$$

$$J_f(x) = \begin{bmatrix}
\frac{\partial z_1}{\partial x_1} & \frac{\partial z_1}{\partial x_2} \\
\frac{\partial z_2}{\partial x_1} & \frac{\partial z_2}{\partial x_2}
\end{bmatrix} = \begin{bmatrix} 2x_1x_2 & x_1^2 \\ 3 & \cos x_2 \end{bmatrix}
$$


## Normalizing flows {auto-animate=true}

- Given samples $X$
- Find $f$ such that $f(X) = Z \sim \text{Normal}(0, \text{I})$
- "$f$ *normalizes* $X$"


## Normalizing flows {auto-animate=true}

- Given samples $X$
- Find $f$ such that $f(X) = Z \sim \text{Normal}(0, \text{I})$
- "$f$ *normalizes* $X$"

:::: {.columns}

:::{.column width="60%"}
#### Density evaluation 

$$
p_X(x) = p_Z(f(x)) \left| \det{J}_f(x) \right|
$$
:::
::::

## Normalizing flows {auto-animate=true}

- Given samples $X$
- Find $f$ such that $f(X) = Z \sim \text{Normal}(0, \text{I})$
- "$f$ *normalizes* $X$"

:::: {.columns}

:::{.column width="60%"}
#### Density evaluation

$$
p_X(x) = p_Z(f(x)) \left| \det{J}_f(x) \right|
$$
:::

:::{.column width="40%"}
#### Sampling

$$
\begin{aligned}
Z &\sim \text{Normal}(0, \text{I}) \\ 
X & = f^{-1}(Z)
\end{aligned}
$$
:::
::::

## Training

Via maximum likelihood (or rather: negative log likelihood)

$$
\arg \min_\phi - \sum_{i=1}^n \log p_Z(f(x_i \mid \phi)) + \log \left| \det{J}_f(x_i \mid \phi) \right|,
$$
where $\phi$ are trainable weights of a neural network that implements $f$.

## Challenge {.incremental}

Flow $f$ is a parametric function that:

  - is invertible ($f^{-1}$ exists)
  - is differentiable
  - has a Jacobian determinant $\left| J_f(x_i \mid \phi) \right|$ that is computationally efficient
  - expressive enough to represent non-trivial distributions

diffeomorphism, bijection

## Flow composition

Invertible and differentiable functions are "closed" under composition

$$
f = f_L \circ f_{L-1} \circ \dots \circ f_1 \\ 
$$

::: {#fig-flow-composition layout="[[-10, 1, -10, 1, -10, 1, -10], [10, 1, 10, 1, 10, 1, 10]]"}

$f_1$ 

$f_2$ 

$f_3$ 


![](../figures/normalizing-flow/x_1.svg)

$\rightarrow$ 

![](../figures/normalizing-flow/x_2.svg)

$\rightarrow$

![](../figures/normalizing-flow/x_3.svg)

$\rightarrow$

![](../figures/normalizing-flow/x_4.svg)

Flow composition in forward direction
:::

## Flow composition - inverse

To invert a flow composition, we invert individual flows and run them in the opposite order

$$
f^{-1} = f_1^{-1} \circ f_2 ^{-1} \circ \dots \circ f_L^{-1} \\ 
$$

::: {#fig-flow-composition layout="[[-10, 1, -10, 1, -10, 1, -10], [10, 1, 10, 1, 10, 1, 10]]"}

$f_1^{-1}$ 

$f_2^{-1}$ 

$f_3^{-1}$ 


![](../figures/normalizing-flow/x_1.svg)

$\leftarrow$ 

![](../figures/normalizing-flow/x_2.svg)

$\leftarrow$

![](../figures/normalizing-flow/x_3.svg)

$\leftarrow$

![](../figures/normalizing-flow/x_4.svg)

Flow composition in backward (inverse) direction
:::

## Flow composition - Jacobian

- Chain rule
$$
\left| J_f(x) \right| = \left| \prod_{l=1}^L J_{f_l}(x)\right| = \prod_{l=1}^L \left| J_{f_l}(x)\right|
$$

- if we have a Jacobian for each individual transformation, then we have a Jacobian for their composition
$$
\arg \min_\phi \sum_{i=1}^n \log p_Z(f(x_i \mid \phi)) + \sum_{l=1}^L \log \left| \det{J}_{f_l}(x_i \mid \phi) \right|
$$

## Linear flow

:::{.incremental}
$$
f(x) = Ax + b
$$

- inverse: $f^{-1}(z) = A^{-1}(x - b)$
- Jacobian: $\left| J_f(x) \right| = \left| A \right|$

- Limitations:
  1. Not expressive (composition of linear functions is a linear function)
  2. Jacobian/inverse may be in $\mathcal{O}(p^3)$
:::

## Coupling flows

- *Increasing* expresiveness while potentially *decreasing* computational costs
- A *coupling flow* is a way to construct non-linear flows

1. Split the data in two disjoint subsets: $x = (x_A, x_B)$
2. Compute parameters conditionally on one subset: $\theta(x_A)$
3. Apply transformation to the other subset: $z_B = f(x_B \mid \theta(x_A))$
4. Concatenate $z = (x_A, z_B)$

## Coupling flow: forward

```{mermaid}
stateDiagram
  direction LR
  [*] --> $$x$$
  state Split {
    $$x$$ --> $$x_A$$
    $$x$$ --> $$x_B$$
  }
  $$x_A$$ --> $$z_A$$
  $$x_A$$ --> $$\theta(x_A)$$
  $$\theta(x_A)$$ --> $$f(x_B\mid\theta(x_A))$$
  $$x_B$$ --> $$f(x_B\mid\theta(x_A))$$
  $$f(x_B\mid\theta(x_A))$$ --> $$z_B$$
  state Concatenate {
    $$z_A$$ --> $$z$$
    $$z_B$$ --> $$z$$
  }
  $$z$$ --> [*]
```


## Coupling flow: backward

## Coupling flow trick

- Jacobian

$$ J_f = 
\begin{bmatrix}
\text{I} & 0 \\
\frac{\partial}{\partial x_A}f(x_B \mid \theta(x_A)) & J_f(x_B \mid \theta(x_A))
\end{bmatrix}
$$

- Determinant

$$
\det{J}_f = \det(\text{I}) \times \det{J}_f(x_B \mid \theta(x_A)) = \det{J}_f(x_B \mid \theta(x_A))
$$


## Coupling flow trick

- $\theta(x_A)$ can be arbitrarily complex
  - non-linear,...
  - $\rightarrow$ neural network
- $f(x_B\mid\theta(x_A))$ needs to be differentiable and invertible
  - easy to calculate determinant Jacobian... 


## Affine coupling

:::{.incremental}
- $\theta(x_A)$: Trainable coupling networks
  - Output: Shift $\mu$ and scale $\sigma$ 
- Linear (affine) transform function $f(x_B\mid\theta(x_A)) = \frac{x_B - \mu(x_A)}{\sigma(x_A)}$

- log Jacobian: $-\log{\sigma(x_A)}$
:::

## Exercise - Moons

:::: {.columns}

::: {.column width="50%"}
### Forward

{{< video ../exercises/normalizing-flow-forward.mp4 width="500" height="350">}}
:::

::: {.column width="50%"}
### Backward

{{< video ../exercises/normalizing-flow-backward.mp4 width="500" height="350">}}
:::

::::

# Flow matching

## Flow matching

