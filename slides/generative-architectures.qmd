---
title: "Generative architectures"
format:
    revealjs: 
        theme: default
        slide-number: true
        html-math-method: mathjax
---

## Probabilistic generative models

We have samples of a random variable X, and we are interested in learning (approximating) it's probability distribution $p_X(X)$

Uses:

- **Evaluate** the density $p_X(X)$
- **Sample** new data from the distribution, $X \sim p_X(X)$

## Example: Mixture model

- The target distribution is modeled as a weighted sum of multiple simpler (e.g., Gaussian) distributions
  - $p_X(X) = \sum_k^K w_k p_k(X)$
- Sampling and evaluating is straightforward
- Theoretically can represent any distribution
- Practically, does not scale well

## Other architectures

- Markov random fields
- Generative adversarial networks (GAN)
- Variational autoencoders (VAE)
- Diffusion models
- **Normalizing flows**
- **Flow matching**

# Normalizing flows

## Normalizing flows

